{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR 10 Compression + Split Resnet\n",
    "\n",
    "We want to find out how to maximize the perforamnce given a total amount of memory. \n",
    "\n",
    "Chart Layout:\n",
    "\n",
    "* x-axis: number of memory-slots\n",
    "* y-axis: model acc\n",
    "* legend: different models\n",
    "\n",
    "The following experiments might be interesting: How much do different compression techniques change the performance of the architecture?\n",
    "\n",
    "1. (CutR/ Nothing) + Nothing + SplitR (Baseline)\n",
    "    * already performed `split_resnet_res_con_exp.ipynb`\n",
    "2. (CutR/ Nothing) + Thinning + SplitR\n",
    "3. (CutR/ Nothing) + Quantization + SplitR\n",
    "    * 3b. (CutR/ Nothing) + transfer-quantization + SplitR\n",
    "4. (CutR/ Nothing) + Sparse Coding + SplitR\n",
    "5. (No encoding) + Convolutional Autoencoder + SplitR\n",
    "6. (CutR/ Nothing) + Fully Connected Autoencoder + SplitR\n",
    "\n",
    "## Note about the sizes\n",
    "\n",
    "CIFAR10-Dataset\n",
    "\n",
    "| cut...                    | output shape | output size | bits (bytes) to adress output coordinates |\n",
    "|---------------------------|--------------|-------------|-------------------------------------------|\n",
    "| after Block 1             | 8x8x64       |  4096       | 12 (2)                                    |\n",
    "| after Block 2             | 4x4x128      |  2048       | 11 (2)                                    |\n",
    "| after Block 3             | 2x2x256      |  1024       | 10 (2)                                    |\n",
    "| after Block 4             | 1x1x512      |   512       |  9 (2)                                    |\n",
    "\n",
    "We assume\n",
    "\n",
    "* one float takes up 4 Byte of Memory\n",
    "* uints are used for output-coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_BASE_SIZE = 2**10\n",
    "BACKBONE_BLOCK = 3\n",
    "SEED = 0\n",
    "\n",
    "OUTPUT_SIZE_ENCODED = 1024\n",
    "OUTPUT_SIZE_UNENCODED = 32*32*3\n",
    "FLOAT_SIZE_BYTE = 4\n",
    "UINT_SIZE_BYTE = 1\n",
    "COORDINATE_SIZE_BYTE = 2\n",
    "\n",
    "LOG_DIR = '/home/marwei/code/EncodedGDumb/logs/'\n",
    "DATA_DIR = '/daten/marwei/pytorch'\n",
    "\n",
    "TOTAL_DS_SIZE = 50000   # number of samples in cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "memory_size_byte = 10 * 1024 * 1024\n",
    "\n",
    "def write_save(name, contents):\n",
    "    out_path = Path('..', 'scripts', name).resolve()\n",
    "    if out_path.exists():\n",
    "        print('File already exits, nothing has been overwritten')\n",
    "    else:\n",
    "        with open(out_path, 'w') as f:\n",
    "            f.write(contents)\n",
    "        \n",
    "def write_save(name, contents):\n",
    "    out_path = Path('..', 'scripts', name).resolve()\n",
    "    with open(out_path, 'w') as f:\n",
    "        f.write(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation 1: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "n_memory_samples = floor(memory_size_byte / (OUTPUT_SIZE_ENCODED * FLOAT_SIZE_BYTE))\n",
    "print(n_memory_samples)\n",
    "assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "n_enc = f\"cifar10_m{n_memory_samples}_cutr{BACKBONE_BLOCK}_splitr{BACKBONE_BLOCK}__s{SEED}\"\n",
    "l_enc =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"cutr\",\n",
    "          \"--encoding_block\", str(BACKBONE_BLOCK),\n",
    "          \"--compressor\", \"none\",\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--backbone_block\", str(BACKBONE_BLOCK),\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n_enc]\n",
    "exps_base = [\" \".join(l_enc)]\n",
    "\n",
    "del n_memory_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_memory_samples = floor(memory_size_byte / (OUTPUT_SIZE_UNENCODED * UINT_SIZE_BYTE))\n",
    "print(n_memory_samples)\n",
    "assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "\n",
    "n_unenc = f\"cifar10_m{n_memory_samples}_resnet__s{SEED}\"\n",
    "l_unenc =  [\"python3 src/main.py\",\n",
    "            \"--dataset\", \"CIFAR10\",\n",
    "            \"--num_classes_per_task\", \"2\",\n",
    "            \"--num_tasks\", \"5\",\n",
    "            \"--seed\", str(SEED),\n",
    "            \"--memory_size\", str(n_memory_samples),\n",
    "            \"--num_passes\", \"128\",\n",
    "            \"--sampler\", \"greedy_sampler\",\n",
    "            \"--encoder\", \"none\",\n",
    "            \"--compressor\", \"none\",\n",
    "            \"--backbone\", \"resnet\",\n",
    "            \"--backbone_block\", \"0\",\n",
    "            \"--data_dir\", DATA_DIR,\n",
    "            \"--log_dir\", LOG_DIR,\n",
    "            \"--exp_name\", n_unenc]\n",
    "exps_base.append(\" \".join(l_unenc))\n",
    "\n",
    "del n_memory_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_save('cifar10_splirR_compression_base.sh', '\\n'.join(exps_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation 2: Thinning\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "compression_factors = [0.5, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "exps_thinning = []\n",
    "for this_compression_factor in compression_factors:\n",
    "\n",
    "    n_elements_per_sample = floor(OUTPUT_SIZE_ENCODED * (1-this_compression_factor))\n",
    "    sample_size_byte = n_elements_per_sample * FLOAT_SIZE_BYTE + n_elements_per_sample * COORDINATE_SIZE_BYTE\n",
    "    n_memory_samples = floor(memory_size_byte / sample_size_byte)\n",
    "    print(n_memory_samples)\n",
    "    assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "    \n",
    "    n = f\"cifar10_m{n_memory_samples}_cutr{BACKBONE_BLOCK}_thinning{int(this_compression_factor*100)}_splitr{BACKBONE_BLOCK}__s{SEED}\"\n",
    "    l =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"cutr\",\n",
    "          \"--encoding_block\", str(BACKBONE_BLOCK),\n",
    "          \"--compressor\", \"thinning\",\n",
    "          \"--compression_factor\", str(this_compression_factor),\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--backbone_block\", str(BACKBONE_BLOCK),\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n]\n",
    "    exps_thinning.append(\" \".join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_compression_factor in compression_factors:\n",
    "\n",
    "    n_elements_per_sample = floor(OUTPUT_SIZE_UNENCODED * (1-this_compression_factor))\n",
    "    sample_size_byte = n_elements_per_sample * UINT_SIZE_BYTE + n_elements_per_sample * COORDINATE_SIZE_BYTE\n",
    "    n_memory_samples = floor(memory_size_byte / sample_size_byte)\n",
    "    print(n_memory_samples)\n",
    "    assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "\n",
    "    n = f\"cifar10_m{n_memory_samples}_thinning{int(this_compression_factor*100)}_resnet__s{SEED}\"\n",
    "    l =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"none\",\n",
    "          \"--compressor\", \"thinning\",\n",
    "          \"--compression_factor\", str(this_compression_factor),\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n]\n",
    "    exps_thinning.append(\" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_save('cifar10_splitR_compression_thinning.sh', '\\n'.join(exps_thinning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation 3: local Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states_list = [2, 4, 8, 16, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2, floor, ceil\n",
    "\n",
    "exps_quantization = []\n",
    "for n_states in n_states_list:\n",
    "\n",
    "    bytes_quantil_mids = n_states * FLOAT_SIZE_BYTE\n",
    "    bit_for_compressed_number = ceil(log2(n_states))\n",
    "    sample_size_byte = ceil(OUTPUT_SIZE_ENCODED * bit_for_compressed_number / 8) + bytes_quantil_mids\n",
    "    n_memory_samples = floor(memory_size_byte / sample_size_byte)\n",
    "    assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "    \n",
    "    n = f\"cifar10_m{n_memory_samples}_cutr{BACKBONE_BLOCK}_quantization{n_states}_splitr{BACKBONE_BLOCK}__s{SEED}\"\n",
    "    l =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"cutr\",\n",
    "          \"--encoding_block\", str(BACKBONE_BLOCK),\n",
    "          \"--compressor\", \"quantization\",\n",
    "          \"--strategy\", \"local\",\n",
    "          \"--n_states\", str(n_states),\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--backbone_block\", str(BACKBONE_BLOCK),\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n]\n",
    "    exps_quantization.append(\" \".join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_states in n_states_list:\n",
    "\n",
    "    bytes_quantil_mids = n_states * UINT_SIZE_BYTE\n",
    "    bit_for_compressed_number = ceil(log2(n_states))\n",
    "    sample_size_byte = ceil(OUTPUT_SIZE_UNENCODED * bit_for_compressed_number / 8) + bytes_quantil_mids\n",
    "    n_memory_samples = floor(memory_size_byte / sample_size_byte)\n",
    "    assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "    \n",
    "    n = f\"cifar10_m{n_memory_samples}_quantization{n_states}_resnet__s{SEED}\"\n",
    "    l =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"none\",\n",
    "          \"--compressor\", \"quantization\",\n",
    "          \"--strategy\", \"local\",\n",
    "          \"--n_states\", str(n_states),\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n]\n",
    "    exps_quantization.append(\" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_save('cifar10_splitR_compression_quantization_local.sh', '\\n'.join(exps_quantization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation 3b: Transfer Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states_list = [2, 4, 8, 16, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2, floor, ceil\n",
    "\n",
    "exps_quantization = []\n",
    "for n_states in n_states_list:\n",
    "\n",
    "    available_mem = memory_size_byte - n_states * FLOAT_SIZE_BYTE\n",
    "    bit_for_compressed_number = ceil(log2(n_states))\n",
    "    sample_size_byte = ceil(OUTPUT_SIZE_ENCODED * bit_for_compressed_number / 8)\n",
    "    n_memory_samples = floor(available_mem / sample_size_byte)\n",
    "    print(n_memory_samples)\n",
    "    assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "    \n",
    "    n = f\"cifar10_m{n_memory_samples}_cutr{BACKBONE_BLOCK}_quantization{n_states}_transTinyImagenet_splitr{BACKBONE_BLOCK}__s{SEED}\"\n",
    "    l =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"cutr\",\n",
    "          \"--encoding_block\", str(BACKBONE_BLOCK),\n",
    "          \"--compressor\", \"quantization\",\n",
    "          \"--n_states\", str(n_states),\n",
    "          \"--strategy\", \"tiny_imagenet_transfer\",\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--backbone_block\", str(BACKBONE_BLOCK),\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n]\n",
    "    exps_quantization.append(\" \".join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_states in n_states_list:\n",
    "\n",
    "    available_mem = memory_size_byte - n_states * FLOAT_SIZE_BYTE\n",
    "    bit_for_compressed_number = ceil(log2(n_states))\n",
    "    sample_size_byte = ceil(OUTPUT_SIZE_UNENCODED * bit_for_compressed_number / 8)\n",
    "    n_memory_samples = floor(available_mem / sample_size_byte)\n",
    "    print(n_memory_samples)\n",
    "    assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "    \n",
    "    n = f\"cifar10_m{n_memory_samples}_quantization{n_states}_transTinyImagenet_resnet__s{SEED}\"\n",
    "    l =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"none\",\n",
    "          \"--compressor\", \"quantization\",\n",
    "          \"--n_states\", str(n_states),\n",
    "          \"--strategy\", \"tiny_imagenet_transfer\",\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n]\n",
    "    exps_quantization.append(\" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_save('cifar10_splitR_compression_quantization_trans.sh', '\\n'.join(exps_quantization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation 5: Conv Autoencoder\n",
    "\n",
    "We do not encode the sample before compression because the spacial dimensions will be very low so we cannot perform convolution and pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sizes = [1, 2, 4, 8, 16]\n",
    "\n",
    "AE_SIZE = {\n",
    "    1: 0.00452423095703125,\n",
    "    2: 0.0056304931640625,\n",
    "    4: 0.007843017578125,\n",
    "    8: 0.01226806640625,\n",
    "    16: 0.0211181640625,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "exps_autoencoder = []\n",
    "for latent_size in latent_sizes:\n",
    "    n_numbers = 8*8*latent_size\n",
    "    byte_per_sample = n_numbers * FLOAT_SIZE_BYTE\n",
    "    n_memory_samples = floor((memory_size_byte - AE_SIZE[latent_size]) / byte_per_sample)\n",
    "    assert(n_memory_samples < TOTAL_DS_SIZE)\n",
    "    print(n_memory_samples)\n",
    "    n = f\"cifar10_m{n_memory_samples}_ae{latent_size}_resnet__s{SEED}\"\n",
    "    l =  [\"python3 src/main.py\",\n",
    "          \"--dataset\", \"CIFAR10\",\n",
    "          \"--num_classes_per_task\", \"2\",\n",
    "          \"--num_tasks\", \"5\",\n",
    "          \"--seed\", str(SEED),\n",
    "          \"--memory_size\", str(n_memory_samples),\n",
    "          \"--num_passes\", \"128\",\n",
    "          \"--sampler\", \"greedy_sampler\",\n",
    "          \"--encoder\", \"none\",\n",
    "          \"--compressor\", \"autoencoder\",\n",
    "          \"--latent_channels\", str(latent_size),\n",
    "          \"--backbone\", \"resnet\",\n",
    "          \"--data_dir\", DATA_DIR,\n",
    "          \"--log_dir\", LOG_DIR,\n",
    "          \"--exp_name\", n]\n",
    "    exps_autoencoder.append(\" \".join(l))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_save(\"cifar10_splitR_compression_convae.sh\", '\\n'.join(exps_autoencoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation 6: Fully Connected Autoencoder\n",
    "\n",
    "Note:\n",
    "\n",
    "* The autoencoder size when compressor==none is bigger than the abailable memory of 4 MiB\n",
    "* a bottleneck_size=2 produces more samples then there are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_sizes = [2, 4, 8, 16, 32, 64]\n",
    "encoders = ['none', 'cutr3']\n",
    "\n",
    "SIZE_FCAE_NONE_MB = {\n",
    "    64: 52.08887481689453,\n",
    "    32: 43.0804443359375,\n",
    "    16: 36.295753479003906,\n",
    "    8: 30.972145080566406,\n",
    "    4: 26.713584899902344,\n",
    "    2: 23.205177307128906,\n",
    "}\n",
    "SIZE_FCAE_CUTR_MB = {\n",
    "    64: 8.331954956054688,\n",
    "    32: 6.5640106201171875,\n",
    "    16: 5.3397216796875,\n",
    "    8: 4.450263977050781,\n",
    "    4: 3.7687225341796875,\n",
    "    2: 3.2361679077148438,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "exps_fcae = []\n",
    "\n",
    "for encoder in encoders:\n",
    "    for bottleneck_size in bottleneck_sizes:\n",
    "        if encoder == 'none':\n",
    "            available_mem_byte = memory_size_byte - SIZE_FCAE_NONE_MB[bottleneck_size] * 1024**2\n",
    "            byte_per_sample = bottleneck_size * FLOAT_SIZE_BYTE\n",
    "            n_memory_samples = floor(available_mem_byte / byte_per_sample)\n",
    "            n = f\"cifar10_m{n_memory_samples}_fcae{bottleneck_size}_resnet__s{SEED}\"\n",
    "            e = [\n",
    "                \"--encoder\", \"none\",\n",
    "                \"--backbone\", \"resnet\",\n",
    "                \"--exp_name\", n\n",
    "            ]\n",
    "        elif encoder == 'cutr3':\n",
    "            available_mem_byte = memory_size_byte - SIZE_FCAE_CUTR_MB[bottleneck_size] * 1024**2\n",
    "            byte_per_sample = bottleneck_size * FLOAT_SIZE_BYTE\n",
    "            n_memory_samples = floor(available_mem_byte / byte_per_sample)\n",
    "            n = f\"cifar10_m{n_memory_samples}_cutr{BACKBONE_BLOCK}_fcae{bottleneck_size}_resnet__s{SEED}\"\n",
    "            e = [\n",
    "                \"--encoder\", \"cutr\",\n",
    "                \"--encoding_block\", str(BACKBONE_BLOCK),\n",
    "                \"--backbone\", \"resnet\",\n",
    "                \"--backbone_block\", str(BACKBONE_BLOCK),\n",
    "                \"--exp_name\", n        \n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError('Unknown Encoder')\n",
    "\n",
    "        if available_mem_byte <= 0:\n",
    "            print('Model to large')\n",
    "            continue\n",
    "        if n_memory_samples > TOTAL_DS_SIZE:\n",
    "            print(f'Not enough elements in dataset for {bottleneck_size=}')\n",
    "            continue\n",
    "        print(n_memory_samples)\n",
    "\n",
    "        l =  [\"python3 src/main.py\",\n",
    "            \"--dataset\", \"CIFAR10\",\n",
    "            \"--num_classes_per_task\", \"2\",\n",
    "            \"--num_tasks\", \"5\",\n",
    "            \"--seed\", str(SEED),\n",
    "            \"--memory_size\", str(n_memory_samples),\n",
    "            \"--num_passes\", \"128\",\n",
    "            \"--sampler\", \"greedy_sampler\",\n",
    "            \"--compressor\", \"fcae\",\n",
    "            \"--data_dir\", DATA_DIR,\n",
    "            \"--log_dir\", LOG_DIR,\n",
    "            \"--bottleneck_neurons\", str(bottleneck_size)\n",
    "        ] + e\n",
    "\n",
    "        exps_fcae.append(' '.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_save('cifar10_splitR_compression_fcae.sh', \"\\n\".join(exps_fcae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "parent_dir = Path('/home/marwei/code/Archived_Logs/cifar10_compression_cutr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "log_paths = [Path(d, 'checkpoint.log') for d in parent_dir.glob('*')]\n",
    "c = []\n",
    "\n",
    "for exp in log_paths:\n",
    "    with open(exp) as infile:\n",
    "        loglines = infile.read().splitlines()\n",
    "\n",
    "    mem_size = int(re.findall(r\"memory_size=(\\d+)\", loglines[0])[0])\n",
    "    try:\n",
    "        final_acc = float(re.findall(r\"Acc: \\[(.*?)\\]\", loglines[-1])[0])\n",
    "    except IndexError:\n",
    "        print(f'could not read {d}')\n",
    "    compressor = re.findall(r\"compressor=\\'(.*?)\\'\", loglines[0])[0]\n",
    "    encoder = re.findall(r\"encoder=\\'(.*?)\\'\", loglines[0])[0]\n",
    "\n",
    "    if compressor == 'thinning':\n",
    "        compressor_param = float(re.findall(r\"compression_factor=(.*?),\", loglines[0])[0])\n",
    "        compressor_name = 'Thinning'\n",
    "    elif compressor == 'quantization':\n",
    "        compressor_param = int(re.findall(r\"n_states=(\\d+)\", loglines[0])[0])\n",
    "        try:\n",
    "            strategy = re.findall(r\"strategy=\\'(.*?)\\'\", loglines[0])[0]\n",
    "        except IndexError:\n",
    "            strategy = 'local'\n",
    "        \n",
    "        if strategy == 'tiny_imagenet_transfer':\n",
    "            compressor_name = 'Quantization (transfer)'\n",
    "        elif strategy == 'local':\n",
    "            compressor_name = 'Quantization (local)'\n",
    "        else:\n",
    "            raise ValueError('Unknown Strategy')\n",
    "    elif compressor == 'autoencoder' or compressor == 'convae':\n",
    "        compressor_param = int(re.findall(r\"latent_channels=(\\d+)\", loglines[0])[0])\n",
    "        compressor = 'convae'\n",
    "        compressor_name = 'Conv. Autoencoder'\n",
    "    elif compressor == 'fcae':\n",
    "        compressor_param = int(re.findall(r\"bottleneck_neurons=(\\d+)\", loglines[0])[0])\n",
    "        compressor_name = 'FC Autoencoder'\n",
    "    elif compressor == 'none':\n",
    "        compressor_param = ''\n",
    "        compressor_name = 'No Compression'\n",
    "    else:\n",
    "        raise ValueError(f'Unknown Compressor: {compressor}')\n",
    "\n",
    "    if encoder == 'cutr':\n",
    "        encoding_block = int(re.findall(r\"encoding_block=(\\d+)\", loglines[0])[0])\n",
    "        encoder_name = f'CutR18({encoding_block})'\n",
    "    elif encoder == 'none':\n",
    "        encoder_name = 'ResNet-18'\n",
    "    else:\n",
    "        raise ValueError('Unknown Encoder')\n",
    "\n",
    "\n",
    "    c.append({\n",
    "        'mem_size': mem_size,\n",
    "        'final_acc': final_acc,\n",
    "        'compressor': compressor,\n",
    "        'compressor_name': compressor_name,\n",
    "        'annotation': compressor_param,\n",
    "        'encoder': encoder,\n",
    "        'encoder_name': encoder_name,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_records(c)\n",
    "\n",
    "df.loc[(df['encoder']=='none') & (df['compressor']=='none'), 'annotation'] = 'GDumb'\n",
    "\n",
    "df.loc[df['compressor'] == 'none', 'order_comp'] = 0\n",
    "df.loc[df['compressor'] == 'thinning', 'order_comp'] = 1\n",
    "df.loc[df['compressor'] == 'quantization local', 'order_comp'] = 2\n",
    "df.loc[df['compressor'] == 'quantization transfer', 'order_comp'] = 3\n",
    "df.loc[df['compressor'] == 'convae', 'order_comp'] = 4\n",
    "df.loc[df['compressor'] == 'fcae', 'order_comp'] = 5\n",
    "\n",
    "df.loc[df['encoder'] == 'none', 'order_enc'] = 0\n",
    "df.loc[df['encoder'] == 'cutr', 'order_enc'] = 1\n",
    "\n",
    "\n",
    "# df.sort_values(['order_enc', 'order_comp', 'mem_size'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plot_utils import science_config_png, science_config_svg, science_template\n",
    "\n",
    "fig = px.line(\n",
    "    df.sort_values('mem_size'),\n",
    "    x='mem_size',\n",
    "    y='final_acc',\n",
    "    color='compressor_name',\n",
    "    facet_col='encoder_name',\n",
    "    text='annotation',\n",
    "    markers=True,\n",
    "    log_x=True,\n",
    "    template=science_template,\n",
    "    title=f'',\n",
    "    labels={\n",
    "        'mem_size': 'Number of Memory Slots',\n",
    "        'final_acc': 'Accuracy',\n",
    "        'name': 'Name',\n",
    "        'compressor_param': 'Parameter',\n",
    "        'compressor': 'Compressor',\n",
    "        'compressor_name': 'Compressor',\n",
    "        'encoder_name': 'Encoder',\n",
    "        'none': 'no Compression',\n",
    "    },\n",
    "    category_orders={\n",
    "        'compressor_name': ['No Compression', 'Thinning', 'Quantization (local)', 'Quantization (transfer)', 'Conv. Autoencoder', 'FC Autoencoder']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_annotation(a):\n",
    "    text: str = a.text\n",
    "    if '=' in text:\n",
    "        a.update(text=text.split('=')[-1])\n",
    "\n",
    "fig.for_each_annotation(update_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.show(renderer='browser', config={\n",
    "    'displaylogo': False,\n",
    "    'toImageButtonOptions': {\n",
    "        'format': 'svg', # one of png, svg, jpeg, webp\n",
    "        'filename': 'plot',\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_small = px.line(\n",
    "    df,\n",
    "    x='mem_size',\n",
    "    y='final_acc',\n",
    "    color='compressor_name',\n",
    "    facet_col='encoder_name',\n",
    "    markers=True,\n",
    "    log_x=True,\n",
    "    template=science_template,\n",
    "    labels={\n",
    "        'mem_size': 'Number of Memory Slots',\n",
    "        'final_acc': 'Accuracy',\n",
    "        'name': 'Name',\n",
    "        'compressor_param': 'Parameter',\n",
    "        'compressor': 'Compressor',\n",
    "        'compressor_name': '',\n",
    "        'encoder_name': 'Encoder',\n",
    "        'none': 'no Compression',\n",
    "    }\n",
    ")\n",
    "fig_small.for_each_annotation(update_annotation)\n",
    "fig_small.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    y=1.05,\n",
    "    xanchor=\"left\",\n",
    "    x=0\n",
    "))\n",
    "fig_small.show(renderer='browser', config={\n",
    "    'displaylogo': False,\n",
    "    'toImageButtonOptions': {\n",
    "        'format': 'svg', # one of png, svg, jpeg, webp\n",
    "        'filename': 'plot',\n",
    "    }\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
